## ðŸ”’ Safety & Alignment

Frameworks and tools for aligning LLM behavior with safety principles:

- Red-teaming for jailbreaks
- Output filtering (moderation models, regex, classifiers)
- RLHF pipeline structure
- Constitutional AI (Anthropic-style)

ðŸŽ¯ Future: ethical evaluation prompts + moderation checklist
