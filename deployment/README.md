## ðŸš€ Deployment & Optimization

Strategies for deploying LLMs efficiently in real-world applications:

- Quantization: INT8, FP4
- ONNX & Triton for inference speed
- Latency reduction on edge devices
- API wrappers for LLM serving

ðŸš§ In progress: Ray Serve + vLLM example
